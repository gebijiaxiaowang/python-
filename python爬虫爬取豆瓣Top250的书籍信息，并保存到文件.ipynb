{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#爬虫练习python爬虫爬取豆瓣Top250的书籍信息，并保存到文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "resp = requests.get('https://book.douban.com/top250?start=0')\n",
    "soup = BeautifulSoup(resp.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 发出请求获得HTML源码的函数\n",
    "def get_html(url):\n",
    "    # 伪装成浏览器访问\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    resp = requests.get(url, headers=headers).text\n",
    "\n",
    "    return resp\n",
    "# 获得所有页面的函数\n",
    "def all_page():\n",
    "    base_url = 'https://book.douban.com/top250?start='\n",
    "    urllist = []\n",
    "    # 从0到225，间隔25的数组\n",
    "    for page in range(0, 250, 25):\n",
    "        allurl = base_url + str(page)\n",
    "        urllist.append(allurl)\n",
    "\n",
    "    return urllist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存成功。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 解析页面，获得数据信息\n",
    "def html_parse():\n",
    "    # 调用函数，for循环迭代出所有页面\n",
    "    for url in all_page():\n",
    "        # BeautifulSoup的解析\n",
    "        soup = BeautifulSoup(get_html(url), 'lxml')\n",
    "        # 书名\n",
    "        alldiv = soup.find_all('div', class_='pl2')\n",
    "        names = [a.find('a')['title'] for a in alldiv]\n",
    "        # 作者\n",
    "        allp = soup.find_all('p', class_='pl')\n",
    "        authors = [p.get_text() for p in allp]\n",
    "        # 评分\n",
    "        starspan = soup.find_all('span', class_='rating_nums')\n",
    "        scores = [s.get_text() for s in starspan]\n",
    "        # 简介\n",
    "        sumspan = soup.find_all('span', class_='inq')\n",
    "        sums = [i.get_text() for i in sumspan]\n",
    "        for name, author, score, sum in zip(names, authors, scores, sums):\n",
    "            name = '书名：' + str(name) + '\\n'\n",
    "            author = '作者：' + str(author) + '\\n'\n",
    "            score = '评分：' + str(score) + '\\n'\n",
    "            sum = '简介：' + str(sum) + '\\n'\n",
    "            data = name + author + score + sum\n",
    "            # 保存数据\n",
    "            f.writelines(data + '=======================' + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "# 文件名\n",
    "filename = '豆瓣图书Top250.txt'\n",
    "# 保存文件操作\n",
    "f = open(filename, 'w', encoding='utf-8')\n",
    "# 调用函数\n",
    "html_parse()\n",
    "f.close()\n",
    "print('保存成功。')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# find_all()方法，\n",
    "# 注意class是Python关键词，后面要加下划线_：\n",
    "# alldiv = soup.find_all('div', class_='pl2')\n",
    "# for a in alldiv:\n",
    "#     names = a.find('a')['title']\n",
    "#     print('find_all():', names)\n",
    "\n",
    "# find()方法：\n",
    "# alldiv2 = soup.find('div', class_='pl2')\n",
    "# names2 = alldiv2.find('a')['title']\n",
    "# print('find():', names2 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
